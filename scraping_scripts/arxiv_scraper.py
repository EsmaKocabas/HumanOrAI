"""ArXiv scraper â€“ deterministik ÅŸekilde 3000 kayÄ±t hedefler."""

import argparse
import csv
import os
import random
import re
import sys
import time
from typing import Dict, Iterable, List, Optional, Set

import requests
from bs4 import BeautifulSoup

BASE_URL = "https://arxiv.org"
# Alternatif URL'ler (birisi Ã§alÄ±ÅŸmazsa diÄŸeri denenir)
FALLBACK_BASE_URLS = [
    "https://arxiv.org",
    "https://export.arxiv.org",
]
ABS_RE = re.compile(r"/abs/(\d+\.\d+)")
DEFAULT_LICENSE = "arXiv.org perpetual, non-exclusive license"
ABSTRACT_CLEANUP_RE = re.compile(r"^.*?abstract:?\s*", re.IGNORECASE)
LIST_DELAY = (1.5, 3.0)  # reCAPTCHA'dan kaÃ§Ä±nmak iÃ§in dengeli bekleme
ABS_DELAY = (1.0, 2.0)  # Abstract iÃ§in bekleme
MAX_SKIP = 5000  # Makul bir Ã¼st limit (Ã§oÄŸu kategoride 5000'den fazla makale yok)
SKIP_STEP = 50  # ORÄ°JÄ°NAL: Deterministik sÄ±ralama iÃ§in 50 kalmalÄ±
EMPTY_PAGE_LIMIT = 10  # Daha hÄ±zlÄ± kategori geÃ§iÅŸi iÃ§in
# Son 100 veri iÃ§in Ã¶zel ayarlar
FAST_SKIP_STEP = 25  # Son 100 veri iÃ§in daha kÃ¼Ã§Ã¼k adÄ±m
VERY_FAST_SKIP_STEP = 10  # Son 50 veri iÃ§in Ã§ok kÃ¼Ã§Ã¼k adÄ±m
ULTRA_FAST_SKIP_STEP = 5  # Son 50 veri iÃ§in ultra kÃ¼Ã§Ã¼k adÄ±m (maksimum hÄ±z)
EXTREME_SKIP_STEP = 1  # Son 30 veri iÃ§in EXTREME kÃ¼Ã§Ã¼k adÄ±m (BAN RÄ°SKÄ° - maksimum hÄ±z!)
BATCH_SIZE = 25
SAVE_INTERVAL = 30  # Her 30 saniyede bir kaydet (kesilme durumunda)

# Ã‡ok sayÄ±da gerÃ§ekÃ§i User-Agent listesi
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0",
    "Mozilla/5.0 (X11; CrOS x86_64 15359.58.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.134 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

# Ä°lk 8 kategori ORÄ°JÄ°NAL sÄ±rada (deterministik sÄ±ralama iÃ§in)
# Ek kategoriler sona eklendi (daha fazla veri iÃ§in)
CATEGORIES = [
    "cs.AI", "cs.LG", "cs.CL", "cs.CV", "cs.SE", "cs.PL", 
    "stat.ML", "math.OC",  # ORÄ°JÄ°NAL 8 kategori (aynÄ± sÄ±rada)
    # Ek kategoriler (eÄŸer ilk 8'den yeterli veri Ã§ekilemezse)
    "cs.NE", "cs.IR", "cs.CY", "cs.DS", "cs.CR", "cs.CC", 
    "cs.GT", "cs.RO", "cs.SI", "cs.HC", "math.ST", "stat.TH", 
    "eess.AS", "eess.IV", "cs.SY", "cs.ET", "cs.AR", "cs.DB",
    "cs.DC", "cs.DM", "cs.FL", "cs.GL", "cs.GR", "cs.HO",
    "cs.IT", "cs.LO", "cs.MA", "cs.MM", "cs.MS", "cs.NA",
    "cs.NI", "cs.OH", "cs.OS", "cs.SD", "math.PR", "math.SP",
    "math.CO", "math.AP", "math.CA", "math.AT", "math.DG", "math.FA",
    # Daha fazla kategori (200 veri iÃ§in - duplicate'ler kaldÄ±rÄ±ldÄ±)
    "cs.SC", "cs.TC", "math.AG", "math.CT", "math.GN", "math.GR",
    "math.GT", "math.HO", "math.KT", "math.MG", "math.MP", "math.NT",
    "math.QA", "math.RA", "math.RT", "math.SG",     "physics.ao-ph",
    "physics.app-ph", "physics.bio-ph", "physics.class-ph", "physics.comp-ph",
    "physics.data-an", "physics.flu-dyn", "physics.gen-ph", "physics.ins-det",
    # Daha fazla kategori (son 200 veri iÃ§in)
    "physics.optics", "physics.soc-ph", "physics.space-ph", "q-bio.BM",
    "q-bio.CB", "q-bio.GN", "q-bio.MN", "q-bio.NC", "q-bio.OT", "q-bio.PE",
    "q-bio.QM", "q-bio.SC", "q-bio.TO", "q-fin.CP", "q-fin.GN", "q-fin.MF",
    "q-fin.PM", "q-fin.PR", "q-fin.RM", "q-fin.ST", "q-fin.TR", "stat.AP",
    "stat.CO", "stat.ME", "stat.ML", "stat.OT", "eess.SP", "eess.SY",
    "eess.IV", "eess.AS", "astro-ph.CO", "astro-ph.EP", "astro-ph.GA",
    "astro-ph.HE", "astro-ph.IM", "astro-ph.SR", "cond-mat.dis-nn",
    "cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.other",
    "cond-mat.quant-gas", "cond-mat.soft", "cond-mat.stat-mech",
    "cond-mat.str-el", "cond-mat.supr-con", "gr-qc", "hep-ex", "hep-lat",
    "hep-ph", "hep-th", "math-ph", "nlin.AO", "nlin.CD", "nlin.CG",
    "nlin.PS", "nlin.SI", "nucl-ex", "nucl-th"
]


def get_random_headers(referer: str = None) -> Dict[str, str]:
    """Rastgele User-Agent ve gerÃ§ekÃ§i header'lar dÃ¶ndÃ¼rÃ¼r."""
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": random.choice([
            "en-US,en;q=0.9",
            "en-GB,en;q=0.9",
            "en-US,en;q=0.9,tr;q=0.8",
            "en-US,en;q=0.9,de;q=0.8",
        ]),
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin" if referer else "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0",
    }
    if referer:
        headers["Referer"] = referer
    return headers


def wait(delay_range: tuple[float, float]) -> None:
    """Rastgele bir sÃ¼re bekler."""
    time.sleep(random.uniform(*delay_range))


def looks_like_recaptcha(text: str) -> bool:
    """Metinde reCAPTCHA olup olmadÄ±ÄŸÄ±nÄ± kontrol eder."""
    if not text or len(text) < 100:
        return False  # Ã‡ok kÄ±sa ise reCAPTCHA deÄŸil
    text_lower = text.lower()
    # Daha spesifik kontrol - sadece gerÃ§ek reCAPTCHA sayfalarÄ±nÄ± yakala
    captcha_indicators = [
        "recaptcha",
        "i'm not a robot",
        "verify you're human",
        "cloudflare",
        "challenge-platform",
        "cf-challenge",
        "checking your browser",
    ]
    return any(indicator in text_lower for indicator in captcha_indicators)


def fetch_html(session: requests.Session, url: str, retry_count: int = 0) -> Optional[str]:
    """URL'den HTML iÃ§eriÄŸini Ã§eker. reCAPTCHA korumasÄ± ile."""
    max_retries = 5  # ArtÄ±rÄ±ldÄ±: 3 -> 5
    
    try:
        # Her istekte yeni header'lar kullan (referer ile)
        referer = f"{BASE_URL}/list/" if "/list/" in url else BASE_URL
        headers = get_random_headers(referer=referer)
        # Timeout artÄ±rÄ±ldÄ±: 30 -> 60 saniye (yavaÅŸ baÄŸlantÄ±lar iÃ§in)
        resp = session.get(url, headers=headers, timeout=60, allow_redirects=True, verify=True)
        resp.raise_for_status()
        
        # reCAPTCHA kontrolÃ¼
        if looks_like_recaptcha(resp.text):
            if retry_count < max_retries:
                wait_time = (15 + retry_count * 10, 25 + retry_count * 10)
                print(f"âš  reCAPTCHA yakalandÄ± (deneme {retry_count + 1}/{max_retries}), {wait_time[0]:.0f}-{wait_time[1]:.0f} saniye bekleniyor...")
                wait(wait_time)
                # Session cookies'lerini temizle
                session.cookies.clear()
                session.headers.clear()
                session.headers.update(get_random_headers())
                return fetch_html(session, url, retry_count + 1)
            else:
                print("âŒ reCAPTCHA aÅŸÄ±lamadÄ±, istek atlandÄ±.")
                wait((60, 120))  # Ã‡ok uzun bekleme
                return None
        
        # BaÅŸarÄ±lÄ± istek
        return resp.text
        
    except requests.exceptions.Timeout:
        if retry_count < max_retries:
            wait_time = (10 + retry_count * 5, 20 + retry_count * 5)
            print(f"âš  Timeout hatasÄ± (deneme {retry_count + 1}/{max_retries}), {wait_time[0]:.0f}-{wait_time[1]:.0f} saniye bekleniyor...")
            wait(wait_time)
            # Session'Ä± yenile
            session.cookies.clear()
            session.headers.clear()
            session.headers.update(get_random_headers())
            return fetch_html(session, url, retry_count + 1)
        else:
            print(f"âŒ Timeout hatasÄ± aÅŸÄ±lamadÄ±: {url}")
            wait((10, 20))
            return None
            
    except requests.exceptions.ConnectionError as exc:
        if retry_count < max_retries:
            wait_time = (15 + retry_count * 10, 30 + retry_count * 10)
            print(f"âš  BaÄŸlantÄ± hatasÄ± (deneme {retry_count + 1}/{max_retries}): {type(exc).__name__}. {wait_time[0]:.0f}-{wait_time[1]:.0f} saniye bekleniyor...")
            print(f"   Hata detayÄ±: {str(exc)[:100]}")
            wait(wait_time)
            # Session'Ä± tamamen yenile
            session.cookies.clear()
            session.headers.clear()
            session.headers.update(get_random_headers())
            return fetch_html(session, url, retry_count + 1)
        else:
            print(f"âŒ BaÄŸlantÄ± hatasÄ± aÅŸÄ±lamadÄ±: {type(exc).__name__}")
            print(f"   LÃ¼tfen internet baÄŸlantÄ±nÄ±zÄ± kontrol edin veya birkaÃ§ dakika bekleyip tekrar deneyin.")
            wait((30, 60))
            return None
            
    except requests.exceptions.HTTPError as exc:
        # HTTP status code hatalarÄ± (400, 403, 404, 500, vb.)
        status_code = exc.response.status_code if hasattr(exc, 'response') else 'Unknown'
        
        # HTTP 400 iÃ§in Ã¶zel iÅŸleme
        if status_code == 400:
            if retry_count < max_retries:
                wait_time = (30 + retry_count * 15, 60 + retry_count * 15)  # Daha uzun bekleme
                print(f"âš  HTTP 400 Bad Request (deneme {retry_count + 1}/{max_retries}), {wait_time[0]:.0f}-{wait_time[1]:.0f} saniye bekleniyor...")
                print(f"   URL: {url[:80]}...")
                wait(wait_time)
                # Session'Ä± tamamen yeniden oluÅŸtur
                session.close()
                session = requests.Session()
                # Referer header ekle
                referer = f"{BASE_URL}/list/" if "/list/" in url else BASE_URL
                session.headers.update(get_random_headers(referer=referer))
                # URL'yi temizle ve tekrar dene
                return fetch_html(session, url, retry_count + 1)
            else:
                print(f"âŒ HTTP 400 hatasÄ± aÅŸÄ±lamadÄ±: {url}")
                print(f"   Bu kategoriyi atlayÄ±p devam ediliyor...")
                wait((30, 60))
                return None
        elif retry_count < max_retries:
            wait_time = (10 + retry_count * 5, 20 + retry_count * 5)
            print(f"âš  HTTP {status_code} hatasÄ± (deneme {retry_count + 1}/{max_retries}), {wait_time[0]:.0f}-{wait_time[1]:.0f} saniye bekleniyor...")
            wait(wait_time)
            session.cookies.clear()
            session.headers.clear()
            session.headers.update(get_random_headers())
            return fetch_html(session, url, retry_count + 1)
        else:
            print(f"âŒ HTTP {status_code} hatasÄ± aÅŸÄ±lamadÄ±: {url}")
            wait((10, 20))
            return None
            
    except requests.RequestException as exc:
        # DiÄŸer tÃ¼m HTTP hatalarÄ±
        if retry_count < max_retries:
            wait_time = (10 + retry_count * 5, 20 + retry_count * 5)
            error_type = type(exc).__name__
            print(f"âš  HTTP hatasÄ± ({error_type}, deneme {retry_count + 1}/{max_retries}): {str(exc)[:100]}")
            print(f"   {wait_time[0]:.0f}-{wait_time[1]:.0f} saniye bekleniyor...")
            wait(wait_time)
            session.cookies.clear()
            session.headers.clear()
            session.headers.update(get_random_headers())
            return fetch_html(session, url, retry_count + 1)
        else:
            error_type = type(exc).__name__
            print(f"âŒ HTTP hatasÄ± aÅŸÄ±lamadÄ± ({error_type}): {str(exc)[:100]}")
            wait((10, 20))
            return None


def parse_list(html: str) -> List[str]:
    """Liste sayfasÄ±ndan sadece makale ID'lerini Ã§Ä±karÄ±r."""
    soup = BeautifulSoup(html, "html.parser")
    ids = []
    for link in soup.find_all("a", href=True):
        match = ABS_RE.search(link["href"])
        if match:
            pid = match.group(1)
            if pid not in ids:  # Duplicate kontrolÃ¼
                ids.append(pid)
    return ids


def parse_list_with_abstracts(html: str) -> List[Dict[str, str]]:
    """Liste sayfasÄ±ndan makale ID'lerini VE abstract'larÄ± birlikte Ã§Ä±karÄ±r.
    Bu Ã§ok daha hÄ±zlÄ± Ã§Ã¼nkÃ¼ her makale iÃ§in ayrÄ± sayfa aÃ§mamÄ±za gerek kalmaz."""
    soup = BeautifulSoup(html, "html.parser")
    records = []
    
    # ArXiv list sayfalarÄ±nda her makale bir <dl> elementi iÃ§inde
    # Ã–nce class="list-identifier" olanlarÄ± dene, sonra tÃ¼m <dl> elementlerini dene
    dl_elements = soup.find_all("dl", class_="list-identifier")
    if not dl_elements:
        dl_elements = soup.find_all("dl")  # Fallback: tÃ¼m dl elementleri
    
    for dl in dl_elements:
        # ID'yi bul - genellikle <a> tag'inde
        id_link = dl.find("a", href=ABS_RE)
        if not id_link:
            # Alternatif: dt iÃ§inde arayalÄ±m
            dt = dl.find("dt")
            if dt:
                id_link = dt.find("a", href=ABS_RE)
        
        if not id_link:
            continue
            
        match = ABS_RE.search(id_link["href"])
        if not match:
            continue
            
        paper_id = match.group(1)
        url = f"{BASE_URL}/abs/{paper_id}"
        
        # Abstract'i bul - liste sayfasÄ±nda <p class="mathjax"> iÃ§inde olabilir
        abstract_text = None
        
        # Ã–nce <p class="mathjax"> iÃ§inde ara
        mathjax_p = dl.find("p", class_="mathjax")
        if mathjax_p:
            abstract_text = mathjax_p.get_text(separator=" ", strip=True)
        
        # DeÄŸilse, tÃ¼m <dd> elementlerinde ara
        if not abstract_text:
            for dd in dl.find_all("dd"):
                text = dd.get_text(separator=" ", strip=True)
                # "Abstract:" kelimesini iÃ§eren text'i bul
                if "abstract" in text.lower() and len(text) > 50:
                    abstract_text = text
                    break
        
        # Hala bulamadÄ±ysak, tÃ¼m dl iÃ§eriÄŸinde ara
        if not abstract_text:
            all_text = dl.get_text(separator=" ", strip=True)
            # "Abstract:" kelimesinden sonrasÄ±nÄ± al - daha geniÅŸ pattern
            abstract_match = re.search(r"abstract:?\s*(.+?)(?:Subject Classification|Categories:|MSC Class:|arXiv:|$)", 
                                     all_text, re.IGNORECASE | re.DOTALL)
            if abstract_match:
                abstract_text = abstract_match.group(1).strip()
        
        if not abstract_text or len(abstract_text) < 30:
            continue
        
        # "Abstract:" kelimesini ve fazladan boÅŸluklarÄ± temizle
        abstract_text = ABSTRACT_CLEANUP_RE.sub("", abstract_text).strip()
        abstract_text = re.sub(r"\s+", " ", abstract_text)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        
        if len(abstract_text) < 30:
            continue
        
        records.append({
            "paper_id": paper_id,
            "abstract_text": abstract_text,
            "source_url": url,
        })
    
    return records


def fetch_abstract(session: requests.Session, paper_id: str) -> Optional[Dict[str, str]]:
    """Bir makalenin Ã¶zetini Ã§eker."""
    url = f"{BASE_URL}/abs/{paper_id}"
    html = fetch_html(session, url)
    
    if not html or len(html) < 50:
        return None
    
    if looks_like_recaptcha(html):
        return None
    
    try:
        soup = BeautifulSoup(html, "html.parser")
        
        # Abstract'i bul
        abstract_div = soup.find("blockquote", class_="abstract")
        if not abstract_div:
            # Alternatif yÃ¶ntem: "Abstract:" kelimesinden sonrasÄ±nÄ± al
            abstract_text = ""
            for elem in soup.find_all(["p", "div"]):
                text = elem.get_text()
                if "abstract" in text.lower() and len(text) > 50:
                    # "Abstract:" kelimesini Ã§Ä±kar
                    abstract_text = re.sub(r"^.*?abstract:?\s*", "", text, flags=re.IGNORECASE).strip()
                    break
            
            if not abstract_text:
                return None
        else:
            abstract_text = abstract_div.get_text(separator=" ", strip=True)
            # "Abstract:" kelimesini Ã§Ä±kar
            abstract_text = re.sub(r"^.*?abstract:?\s*", "", abstract_text, flags=re.IGNORECASE).strip()
        
        if len(abstract_text) < 50:
            return None
        
        return {
            "abstract_text": abstract_text,
            "source_url": url,
            "license_info": DEFAULT_LICENSE,
            "label": "Human",
        }
    except Exception as exc:
        print(f"Ã–zet parse hatasÄ± ({paper_id}): {exc}")
        return None


def page_records_from_list(records_from_page: List[Dict[str, str]], existing: Set[str]) -> Iterable[Dict[str, str]]:
    """Liste sayfasÄ±ndan parse edilen kayÄ±tlarÄ± iÅŸler (Ã§ok daha hÄ±zlÄ±)."""
    for idx, rec in enumerate(records_from_page):
        pid = rec["paper_id"]
        if pid in existing:
            continue
        
        # Liste sayfasÄ±ndan zaten abstract'Ä± aldÄ±k, sadece formatla
        existing.add(pid)
        yield {
            "abstract_text": rec["abstract_text"],
            "source_url": rec["source_url"],
            "license_info": DEFAULT_LICENSE,
            "label": "Human",
        }
        
        # Her 5 kayÄ±tta bir bekleme (bot tespitini Ã¶nlemek iÃ§in)
        if (idx + 1) % 5 == 0:
            wait((0.5, 1.0))


def iter_category_historical(session: requests.Session, category: str, existing: Set[str], remaining_needed: int) -> Iterable[Dict[str, str]]:
    """Tarihsel arama: En eski yÄ±llardan baÅŸlayarak geriye doÄŸru gider (son 30 veri iÃ§in)."""
    from datetime import datetime
    
    # En eski ArXiv makaleleri 1991'den baÅŸlÄ±yor
    current_year = datetime.now().year
    start_year = 1991
    
    print(f"  ğŸ“… TARÄ°HSEL ARAMA: {category} iÃ§in {start_year}-{current_year} arasÄ± taranÄ±yor (en eskiden baÅŸlayarak)...")
    
    found_count = 0
    
    # YÄ±llarÄ± geriye doÄŸru tara (en eskiden baÅŸla)
    for year in range(start_year, current_year + 1):
        if found_count >= remaining_needed:
            break
            
        # YÄ±llÄ±k liste URL'i
        url = f"{BASE_URL}/list/{category}/{year}"
        
        wait((0.1, 0.2))  # Minimum bekleme
        
        html = fetch_html(session, url)
        if not html:
            continue
        
        # Liste sayfasÄ±ndan direkt abstract'larÄ± parse et
        records_from_page = parse_list_with_abstracts(html)
        
        if not records_from_page:
            # Fallback: ID'leri al ve ayrÄ± sayfalardan Ã§ek
            ids = parse_list(html)
            for pid in ids:
                if pid in existing:
                    continue
                record = fetch_abstract(session, pid)
                wait((0.1, 0.2))
                if record:
                    existing.add(pid)
                    yield record
                    found_count += 1
                    if found_count >= remaining_needed:
                        print(f"  âœ… Tarihsel arama tamamlandÄ±: {found_count} yeni kayÄ±t bulundu")
                        return
            continue
        
        # Parse edilen kayÄ±tlarÄ± iÅŸle
        for record in page_records_from_list(records_from_page, existing):
            yield record
            found_count += 1
            if found_count >= remaining_needed:
                print(f"  âœ… Tarihsel arama tamamlandÄ±: {found_count} yeni kayÄ±t bulundu")
                return
        
        # AylÄ±k listeleri de dene (daha detaylÄ±) - sadece ilk birkaÃ§ yÄ±l iÃ§in
        if year <= 2000:  # Ä°lk 10 yÄ±l iÃ§in aylÄ±k tarama
            for month in range(1, 13):
                if found_count >= remaining_needed:
                    break
                month_str = f"{year}{month:02d}"
                url_month = f"{BASE_URL}/list/{category}/{month_str}"
                
                wait((0.1, 0.2))
                html_month = fetch_html(session, url_month)
                if not html_month:
                    continue
                
                records_month = parse_list_with_abstracts(html_month)
                if records_month:
                    for record in page_records_from_list(records_month, existing):
                        yield record
                        found_count += 1
                        if found_count >= remaining_needed:
                            print(f"  âœ… Tarihsel arama tamamlandÄ±: {found_count} yeni kayÄ±t bulundu")
                            return
    
    print(f"  ğŸ“… Tarihsel arama tamamlandÄ±: {found_count} yeni kayÄ±t bulundu")


def iter_category(session: requests.Session, category: str, existing: Set[str], remaining_needed: int = None) -> Iterable[Dict[str, str]]:
    """Bir kategorideki tÃ¼m makaleleri iterasyonla dÃ¶ndÃ¼rÃ¼r (optimize edilmiÅŸ - liste sayfalarÄ±ndan direkt parse)."""
    empty_count = 0
    total_new = 0
    consecutive_failures = 0  # ArdÄ±ÅŸÄ±k baÅŸarÄ±sÄ±zlÄ±k sayacÄ±
    http_400_count = 0  # HTTP 400 hatasÄ± sayacÄ±
    
    # Son 40 veri iÃ§in 1 DAKÄ°KALIK HIZLI ALGORÄ°TMA (tarihsel arama atlanÄ±yor - Ã§ok yavaÅŸ!)
    if remaining_needed and remaining_needed <= 40:
        page_limit = 1  # 1 boÅŸ sayfa sonra geÃ§ (maksimum hÄ±z!)
        skip_step = 1  # Her makale (EXTREME!)
        list_delay = (0.05, 0.1)  # MINIMUM bekleme (BAN RÄ°SKÄ° - 1 dakika hedefi!)
        print(f"  âš¡âš¡âš¡ SON {remaining_needed} VERÄ° Ä°Ã‡Ä°N 1 DAKÄ°KALIK HIZLI MOD! skip_step={skip_step}, delay={list_delay}")
    # Son 50 veri iÃ§in ULTRA agresif ayarlar (maksimum hÄ±z!)
    elif remaining_needed and remaining_needed <= 50:
        page_limit = 1  # 1 boÅŸ sayfa sonra geÃ§ (maksimum hÄ±z!)
        skip_step = ULTRA_FAST_SKIP_STEP  # Ultra kÃ¼Ã§Ã¼k adÄ±mlar (5)
        list_delay = (0.2, 0.4)  # Ã‡ok minimum bekleme (maksimum hÄ±z!)
        print(f"  ğŸ”¥ğŸ”¥ğŸ”¥ SON {remaining_needed} VERÄ° Ä°Ã‡Ä°N ULTRA HIZLI MOD! (skip_step={skip_step}, delay={list_delay})")
    # Son 100 veri iÃ§in Ã§ok agresif ayarlar
    elif remaining_needed and remaining_needed <= 100:
        page_limit = 2  # Ã‡ok hÄ±zlÄ± kategori geÃ§iÅŸi
        skip_step = VERY_FAST_SKIP_STEP  # Ã‡ok kÃ¼Ã§Ã¼k adÄ±mlar
        list_delay = (0.5, 1.0)  # Daha kÄ±sa bekleme
        print(f"  ğŸš€ SON {remaining_needed} VERÄ° Ä°Ã‡Ä°N HIZLI MOD AKTÄ°F! (skip_step={skip_step}, delay={list_delay})")
    elif remaining_needed and remaining_needed < 500:
        page_limit = 3  # HÄ±zlÄ± kategori geÃ§iÅŸi
        skip_step = FAST_SKIP_STEP  # KÃ¼Ã§Ã¼k adÄ±mlar
        list_delay = (1.0, 2.0)  # Orta bekleme
        print(f"  âš¡ Az veri kaldÄ± ({remaining_needed}), hÄ±zlÄ± mod (skip_step={skip_step})...")
    else:
        page_limit = EMPTY_PAGE_LIMIT
        skip_step = SKIP_STEP
        list_delay = LIST_DELAY
    
    # Yeni strateji: 1 DAKÄ°KALIK HIZLI ALGORÄ°TMA - Sadece ilk sayfalarÄ± tara (Ã§ok hÄ±zlÄ±!)
    if remaining_needed and remaining_needed <= 40:
        # 1 DAKÄ°KALIK MOD: Sadece her kategorinin ilk 20 sayfasÄ±nÄ± tara (skip=0, 25, 50, 75, ... 475)
        # Bu Ã§ok hÄ±zlÄ± Ã§Ã¼nkÃ¼ sadece en yeni makaleleri tarÄ±yor
        skip_ranges = [
            range(0, 500, 25),  # Ä°lk 20 sayfa (skip=0, 25, 50, 75, ..., 475) - Ã‡OK HIZLI!
        ]
        print(f"  âš¡âš¡âš¡ SON {remaining_needed} VERÄ°: 1 DAKÄ°KALIK HIZLI MOD! Sadece ilk 20 sayfa taranÄ±yor (skip=0-475, step=25)...")
    # Son 50 veri iÃ§in ULTRA geniÅŸ ve detaylÄ± tarama (maksimum kapsama!)
    elif remaining_needed and remaining_needed <= 50:
        # Son 50 veri iÃ§in: ULTRA geniÅŸ aralÄ±k, ultra kÃ¼Ã§Ã¼k adÄ±mlarla, maksimum kapsama
        skip_ranges = [
            range(0, 5000, skip_step),  # En yeni makaleler (Ã§ok geniÅŸ)
            range(5000, 10000, skip_step),  # Orta-eskiler
            range(10000, 15000, skip_step),  # Daha eskiler
            range(15000 - skip_step, 0, -skip_step),  # Geriye doÄŸru (en eski)
            # Ek olarak: Ã§ok fazla rastgele aralÄ±k (maksimum kapsama)
            range(0, 1000, skip_step),
            range(100, 1500, skip_step),
            range(200, 2000, skip_step),
            range(500, 2500, skip_step),
            range(1000, 3500, skip_step),
            range(2000, 4500, skip_step),
            range(3000, 5500, skip_step),
            range(4000, 6500, skip_step),
        ]
        print(f"  ğŸ”¥ SON {remaining_needed} VERÄ°: ULTRA geniÅŸ aralÄ±kta ULTRA detaylÄ± tarama (maksimum hÄ±z!)...")
    # Son 100 veri iÃ§in Ã§ok daha geniÅŸ ve detaylÄ± tarama
    elif remaining_needed and remaining_needed <= 100:
        # Son 100 veri iÃ§in: Ã§ok geniÅŸ aralÄ±k, kÃ¼Ã§Ã¼k adÄ±mlarla
        skip_ranges = [
            range(0, 3000, skip_step),  # En yeni makaleler (geniÅŸ aralÄ±k)
            range(3000, 6000, skip_step),  # Orta-eskiler
            range(6000, 10000, skip_step),  # Daha eskiler
            range(10000 - skip_step, 0, -skip_step),  # Geriye doÄŸru (en eski)
            # Ek olarak: rastgele aralÄ±klar
            range(100, 2000, skip_step),
            range(500, 2500, skip_step),
            range(1000, 3500, skip_step),
        ]
        print(f"  ğŸ“œ SON {remaining_needed} VERÄ°: Ã‡ok geniÅŸ aralÄ±kta detaylÄ± tarama...")
    elif remaining_needed and remaining_needed < 500:
        # Az veri kaldÄ±ÄŸÄ±nda: farklÄ± aralÄ±klarÄ± dene
        skip_ranges = [
            range(0, 2000, skip_step),  # En yeni makaleler
            range(2000, 4000, skip_step),  # Orta-eskiler
            range(4000, MAX_SKIP, skip_step),  # Daha eskiler
            range(MAX_SKIP - skip_step, 0, -skip_step),  # Geriye doÄŸru (en eski)
        ]
        print(f"  ğŸ“œ Az veri kaldÄ± ({remaining_needed}), farklÄ± aralÄ±klarda taranÄ±yor...")
    else:
        # Normal: en yeni makalelerden baÅŸla
        skip_ranges = [range(0, MAX_SKIP, skip_step)]
    
    # TÃ¼m skip aralÄ±klarÄ±nÄ± dene
    for skip_range in skip_ranges:
        if total_new >= (remaining_needed or 1000):  # Yeterli veri bulunduysa dur
            break
    
        for skip in skip_range:
            # HTTP 400 hatasÄ± Ã§ok fazlaysa bu kategoriyi atla
            if http_400_count >= 5:
                print(f"âš  {category}: Ã‡ok fazla HTTP 400 hatasÄ± ({http_400_count}), kategori atlanÄ±yor...")
                return
            
            # URL formatÄ±nÄ± dÃ¼zelt - skip parametresini doÄŸru ekle
            if skip > 0:
                url = f"{BASE_URL}/list/{category}/recent?skip={skip}"
            else:
                url = f"{BASE_URL}/list/{category}/recent"
            
            # Ä°stek Ã¶ncesi bekleme (reCAPTCHA'dan kaÃ§Ä±nmak iÃ§in)
            # Son 40 veri iÃ§in MINIMUM bekleme (1 DAKÄ°KALIK MOD - BAN RÄ°SKÄ°!)
            if remaining_needed and remaining_needed <= 40:
                wait(list_delay)  # EXTREME kÄ±sa bekleme (0.05-0.1 saniye - 1 DAKÄ°KALIK MOD!)
            # Son 50 veri iÃ§in minimum bekleme (maksimum hÄ±z!)
            elif remaining_needed and remaining_needed <= 50:
                wait(list_delay)  # Ultra kÄ±sa bekleme (0.2-0.4 saniye)
            elif remaining_needed and remaining_needed <= 100:
                wait(list_delay)  # KÄ±sa bekleme (0.5-1.0 saniye)
            else:
                wait(LIST_DELAY)
            
            html = fetch_html(session, url)
            
            # HTTP 400 hatasÄ± kontrolÃ¼
            if html is None:
                # fetch_html iÃ§inde HTTP 400 hatasÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
                # (fetch_html None dÃ¶ndÃ¼rÃ¼yorsa ve HTTP 400 ise sayacÄ± artÄ±r)
                http_400_count += 1
            else:
                http_400_count = 0  # BaÅŸarÄ±lÄ± istek sonrasÄ± reset
            
            # EÄŸer baÅŸarÄ±sÄ±z ise, daha uzun bekle
            if not html:
                consecutive_failures += 1
                if consecutive_failures >= 3:
                    wait_time = (30, 60)  # Daha kÄ±sa bekleme
                    print(f"âš  ArdÄ±ÅŸÄ±k {consecutive_failures} baÅŸarÄ±sÄ±z istek! {wait_time[0]:.0f}-{wait_time[1]:.0f} saniye bekleniyor...")
                    wait(wait_time)
                    consecutive_failures = 0
                    # Session cookies'lerini temizle
                    session.cookies.clear()
                    session.headers.clear()
                    session.headers.update(get_random_headers())
            else:
                consecutive_failures = 0  # BaÅŸarÄ±lÄ± istek sonrasÄ± reset
            
            if not html:
                empty_count += 1
                if empty_count >= page_limit:
                    print(f"âš  {category} kategorisinde boÅŸ sayfa limitine ulaÅŸÄ±ldÄ± ({empty_count}/{page_limit}).")
                    break
                continue
            
            # Ã–nce ID'leri parse et
            ids = parse_list(html)
            if not ids:
                empty_count += 1
                if empty_count >= page_limit:
                    print(f"âš  {category} kategorisinde ID bulunamadÄ± (skip={skip}, limit: {page_limit}).")
                    break
                continue
            
            # Liste sayfasÄ±ndan direkt abstract'larÄ± parse et (Ã§ok daha hÄ±zlÄ±!)
            records_from_page = parse_list_with_abstracts(html)
            
            # EÄŸer liste sayfasÄ±ndan parse edilemezse, her makale iÃ§in ayrÄ± sayfa aÃ§ (fallback)
            if not records_from_page:
                print(f"  âš  Liste sayfasÄ±ndan parse edilemedi ({len(ids)} ID bulundu), ayrÄ± sayfalardan Ã§ekiliyor...")
                for pid in ids:
                    if pid in existing:
                        continue
                    record = fetch_abstract(session, pid)
                    wait(ABS_DELAY)
                    if record:
                        existing.add(pid)
                        total_new += 1
                        yield record
                        if total_new % 10 == 0:
                            print(f"  ğŸ“„ {total_new} yeni kayÄ±t eklendi (fallback method)")
                continue
            
            empty_count = 0
            parsed_count = len(records_from_page)
            valid_count = 0
            
            for record in page_records_from_list(records_from_page, existing):
                valid_count += 1
                total_new += 1
                yield record
            
            if parsed_count > 0:
                skipped = parsed_count - valid_count
                # Skip step'i doÄŸru hesapla
                if remaining_needed and remaining_needed <= 40:
                    current_skip_step = 25  # 1 dakikalÄ±k mod iÃ§in 25
                elif remaining_needed and remaining_needed <= 50:
                    current_skip_step = ULTRA_FAST_SKIP_STEP
                elif remaining_needed and remaining_needed <= 100:
                    current_skip_step = VERY_FAST_SKIP_STEP
                elif remaining_needed and remaining_needed < 500:
                    current_skip_step = FAST_SKIP_STEP
                else:
                    current_skip_step = SKIP_STEP
                page_num = skip // current_skip_step + 1
                if valid_count > 0 or skipped > 0:
                    print(f"  ğŸ“„ Sayfa {page_num} (skip={skip}): {valid_count}/{parsed_count} yeni kayÄ±t (atlanan: {skipped})")
            
            # EÄŸer az veri kaldÄ±ysa ve yeni kayÄ±t yoksa, daha hÄ±zlÄ± geÃ§
            if remaining_needed and remaining_needed <= 40:
                # Son 40 veri iÃ§in: HEMEN kategori deÄŸiÅŸtir (1 DAKÄ°KALIK MOD - BAN RÄ°SKÄ°!)
                if valid_count == 0:
                    empty_count += 1
                    if empty_count >= 1:  # HEMEN kategori deÄŸiÅŸtir (1 boÅŸ sayfa yeter!)
                        print(f"  âš¡ {category}: Veri bulunamadÄ±, HEMEN sonraki kategoriye geÃ§iliyor (1 DAKÄ°KALIK MOD!)...")
                        break
                else:
                    empty_count = 0  # Veri bulunduysa reset
            elif remaining_needed and remaining_needed <= 50:
                # Son 50 veri iÃ§in: 1 boÅŸ sayfa sonra aralÄ±ÄŸÄ± deÄŸiÅŸtir (ULTRA hÄ±zlÄ±!)
                if valid_count == 0:
                    empty_count += 1
                    if empty_count >= 1:  # ULTRA hÄ±zlÄ± geÃ§iÅŸ (1 boÅŸ sayfa yeter!)
                        print(f"  âš¡ {category}: Bu aralÄ±kta veri bulunamadÄ±, sonraki aralÄ±ÄŸa geÃ§iliyor (ULTRA HIZLI!)...")
                        break
                else:
                    empty_count = 0  # Veri bulunduysa reset
            elif remaining_needed and remaining_needed <= 100:
                # Son 100 veri iÃ§in: 1-2 boÅŸ sayfa sonra aralÄ±ÄŸÄ± deÄŸiÅŸtir
                if valid_count == 0:
                    empty_count += 1
                    if empty_count >= 2:  # Ã‡ok hÄ±zlÄ± geÃ§iÅŸ
                        print(f"  âš  {category}: Bu aralÄ±kta veri bulunamadÄ±, sonraki aralÄ±ÄŸa geÃ§iliyor...")
                        break
                else:
                    empty_count = 0  # Veri bulunduysa reset
            elif remaining_needed and remaining_needed < 500:
                # EÄŸer 2 sayfa Ã¼st Ã¼ste veri yoksa, bu aralÄ±ÄŸÄ± atla
                if valid_count == 0:
                    empty_count += 1
                    if empty_count >= 2:  # HÄ±zlÄ± geÃ§iÅŸ
                        print(f"  âš  {category}: Bu aralÄ±kta veri bulunamadÄ±, sonraki aralÄ±ÄŸa geÃ§iliyor...")
                        break
                else:
                    empty_count = 0  # Veri bulunduysa reset
            elif valid_count == 0 and skip > 1000:
                print(f"  âš  {category}: Son sayfalarda yeni kayÄ±t bulunamadÄ±, sonraki kategoriye geÃ§iliyor...")
                break


def load_existing_ids(path: str) -> Set[str]:
    """Mevcut CSV dosyasÄ±ndan ID'leri yÃ¼kler."""
    if not os.path.exists(path):
        return set()
    
    ids = set()
    try:
        with open(path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                url = row.get("source_url", "")
                match = ABS_RE.search(url)
                if match:
                    ids.add(match.group(1))
    except Exception as exc:
        print(f"âš  Mevcut dosya okuma hatasÄ±: {exc}")
    
    return ids


def write_csv(path: str, rows: List[Dict[str, str]]) -> None:
    """KayÄ±tlarÄ± CSV dosyasÄ±na yazar."""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    exists = os.path.exists(path)
    mode = "a" if exists else "w"
    
    with open(path, mode, newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, ["abstract_text", "source_url", "license_info", "label"])
        if not exists:
            writer.writeheader()
        writer.writerows(rows)


def record_stream(session: requests.Session, existing: Set[str], target: int):
    """TÃ¼m kategorilerden kayÄ±t akÄ±ÅŸÄ± saÄŸlar (deterministik sÄ±ralama)."""
    total = len(existing)
    remaining = target - total
    print(f"ğŸ“Š Mevcut kayÄ±t sayÄ±sÄ±: {total}, Hedef: {target}, Eksik: {remaining}")
    
    # Son 40 veri iÃ§in 1 DAKÄ°KALIK MOD: TÃ¼m kategorileri Ã§ok hÄ±zlÄ±ca geÃ§
    if remaining <= 40:
        print(f"âš¡âš¡âš¡ 1 DAKÄ°KALIK HIZLI MOD AKTÄ°F! {remaining} veri iÃ§in tÃ¼m kategoriler hÄ±zlÄ±ca taranacak...")
    
    # Kategorileri deterministik sÄ±rada iÅŸle (her zaman aynÄ± sÄ±ra)
    for category in CATEGORIES:
        if total >= target:
            break
        remaining_needed = target - total
        if remaining_needed <= 40:
            print(f" âš¡ {category} kategorisi HIZLI taranÄ±yor... (Kalan: {remaining_needed} kayÄ±t)")
        else:
            print(f" {category} kategorisi taranÄ±yor... (Kalan: {remaining_needed} kayÄ±t)")
        category_count = 0
        for record in iter_category(session, category, existing, remaining_needed):
            total += 1
            category_count += 1
            yield record, total
            
            if total >= target:
                print(f"âœ… Hedef sayÄ±ya ulaÅŸÄ±ldÄ±! {category} kategorisinden {category_count} kayÄ±t alÄ±ndÄ±.")
                return
        
        if category_count > 0:
            print(f"âœ“ {category} kategorisi tamamlandÄ±. {category_count} yeni kayÄ±t eklendi.")
        elif remaining_needed <= 40:
            # 1 dakikalÄ±k modda boÅŸ kategorileri hÄ±zlÄ±ca atla
            pass
    
    if total < target:
        print(f"âš  Hedefe ulaÅŸÄ±lamadÄ±: {total}/{target} kayÄ±t toplandÄ±.")
        print(f"ğŸ’¡ {target - total} kayÄ±t eksik. Tekrar Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda kaldÄ±ÄŸÄ± yerden devam edecek.")
        # Hata verme, mevcut veriyi kabul et


def create_new_session() -> requests.Session:
    """Yeni bir session oluÅŸturur (reCAPTCHA'dan kaÃ§Ä±nmak iÃ§in)."""
    session = requests.Session()
    session.headers.update(get_random_headers())
    return session


def scrape(output: str, target: int) -> None:
    """Ana scraping fonksiyonu."""
    session = create_new_session()
    
    existing = load_existing_ids(output)
    current_total = len(existing)
    
    print(f"ğŸ“Š Mevcut dosyada {current_total} benzersiz ID bulundu")
    
    if current_total >= target:
        print(f"âœ… Zaten {current_total} kayÄ±t mevcut. Hedef: {target}")
        return
    
    print(f"ğŸš€ Scraping baÅŸlatÄ±lÄ±yor... Mevcut: {current_total}, Hedef: {target}")
    remaining = target - current_total
    if remaining > 0:
        # Zaman tahmini (her kayÄ±t iÃ§in ortalama 2-3 saniye)
        estimated_minutes = (remaining * 2.5) / 60
        print(f"ğŸ“ {remaining} yeni kayÄ±t Ã§ekilecek.")
        print(f"  Tahmini sÃ¼re: ~{estimated_minutes:.1f} dakika ({estimated_minutes/60:.1f} saat)")
    
    buffer: List[Dict[str, str]] = []
    last_save = time.time()

    try:
        for record, new_total in record_stream(session, existing, target):
            current_total = new_total
            buffer.append(record)
            
            # Her BATCH_SIZE kayÄ±tta veya SAVE_INTERVAL saniyede bir kaydet (kesilme durumunda koruma)
            if len(buffer) >= BATCH_SIZE or (time.time() - last_save) > SAVE_INTERVAL:
                write_csv(output, buffer)
                buffer.clear()
                last_save = time.time()
                print(f"ğŸ’¾ {current_total}/{target} kayÄ±t kaydedildi.")
            
            if current_total >= target:
                break
            
            # Her 100 kayÄ±tta bir session'Ä± yenile (reCAPTCHA'dan kaÃ§Ä±nmak iÃ§in)
            if current_total % 100 == 0 and current_total > 0:
                print("  ğŸ”„ Session yenileniyor (reCAPTCHA korumasÄ±)...")
                session.close()
                session = create_new_session()
                wait((5, 10))
                
    except KeyboardInterrupt:
        print("\nâš  KullanÄ±cÄ± tarafÄ±ndan durduruldu.")
    except Exception as exc:
        print(f"\nâŒ Hata oluÅŸtu: {exc}")
        import traceback
        traceback.print_exc()
    finally:
        session.close()
        if buffer:
            write_csv(output, buffer)
            print(f"ğŸ’¾ Son kayÄ±tlar kaydedildi. Toplam: {current_total}/{target}")

    print(f"âœ… Ä°ÅŸlem tamamlandÄ±. Toplam {min(current_total, target)} Ã¶zet hazÄ±r.")


def main() -> None:
    """Ana fonksiyon."""
    parser = argparse.ArgumentParser(description="ArXiv scraper - 3000 kayÄ±t hedefler")
    parser.add_argument("--output", default="data/raw/human_abstracts.csv", help="Ã‡Ä±ktÄ± CSV dosyasÄ±")
    parser.add_argument("--target", type=int, default=3000, help="Hedef kayÄ±t sayÄ±sÄ±")
    args = parser.parse_args()
    scrape(args.output, args.target)


if __name__ == "__main__":
    if os.environ.get("HUMANORAI_ALLOW_SCRAPE") != "1":
        print("Scraper devre dÄ±ÅŸÄ±. Ã‡alÄ±ÅŸtÄ±rmak iÃ§in HUMANORAI_ALLOW_SCRAPE=1 ayarla.")
        sys.exit(0)
    main()
